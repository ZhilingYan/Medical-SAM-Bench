<div align="center">

# ğŸ¥ SAMed-2: Selective Memory Enhanced Medical SAM

### *State-of-the-Art Medical Image Segmentation with Memory-Enhanced SAM*

[![Project Page](https://img.shields.io/badge/ğŸŒ_Project-Website-blue)](https://zhilingyan.github.io/Medical-SAM-Bench/)
[![Paper](https://img.shields.io/badge/ğŸ“„_Paper-Arxiv-purple)](https://arxiv.org/abs/xxxx.xxxxx)
[![Demo](https://img.shields.io/badge/ğŸ®_Demo-SliceUI-green)](docs/DEMO.md)
[![Model Zoo](https://img.shields.io/badge/ğŸ¤—_Model-Zoo-red)](docs/MODEL_ZOO.md)
[![License](https://img.shields.io/badge/ğŸ“œ_License-Apache_2.0-yellow.svg)](LICENSE)

**The Official Repository of SAMed-2 & Medical SAM Benchmark**

[**Installation**](#-getting-started) â€¢ [**Quick Start**](#4-quick-start) â€¢ [**Models**](#3-model-zoo) â€¢ [**Results**](#-performance-comparison) â€¢ [**Citation**](#-citation)

</div>

---

## ğŸŒŸ Highlights

<table>
<tr>
<td width="33%" align="center"><b>ğŸ§  Memory-Enhanced</b><br>Selective memory mechanism for robust medical segmentation</td>
<td width="33%" align="center"><b>ğŸ† SOTA Performance</b><br>Best results on multiple medical imaging benchmarks</td>
<td width="33%" align="center"><b>ğŸ”§ Unified Framework</b><br>Fair comparison of all Medical SAM variants</td>
</tr>
</table>

## ğŸ“‹ Abstract

SAMed-2 is a new foundation model for medical image segmentation built upon the SAM-2 architecture. We introduce:
- ğŸ”„ **Temporal adapter** in the image encoder to capture image correlations
- ğŸ’¾ **Confidence-driven memory mechanism** to store high-certainty features for later retrieval
- ğŸ›¡ï¸ **Noise-resistant strategy** to handle large-scale medical datasets
- ğŸ§  **Anti-forgetting mechanism** for new tasks or modalities

<details>
<summary><b>Key Features of This Repository</b></summary>

- âœ… Unified implementation and evaluation framework
- âœ… Fair comparison across all Medical SAM variants
- âœ… Easy-to-use Python API for quick inference
- âœ… Pre-trained models and memory banks available
- âœ… Support for multiple medical imaging modalities

</details>

## ğŸ“° News

> **[07/2025]** ğŸ® Interactive demo tool released - try SAMed-2 on your medical images!  
> **[06/2025]** ğŸ‰ SAMed-2 accepted by MICCAI 2025!  
> **[06/2025]** ğŸš€ Initial release of SAMed-2!

## ğŸ“œ Code License

This project is released under the [Apache 2.0 license](LICENSE).

## ğŸš€ Getting Started

### 1. Installation

**Linux Environment**

Clone this repository and navigate to the folder:

```bash
git clone https://github.com/ZhilingYan/Medical-SAM-Bench.git
cd Medical-SAM-Bench
```

### 2. Install Package

```bash
# Create a new conda environment
conda create -n samed2 python=3.10 -y
conda activate samed2

# Install PyTorch (adjust cuda version as needed)
pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu118

# Install other requirements
pip install -r requirements.txt
```

### 3. Model Zoo

<div align="center">

| Model | Architecture | Medical<br>Fine-tuned | Performance | Download |
|:-----:|:------------:|:---------------------:|:-----------:|:--------:|
| **SAMed-2** â­ | SAM2-Hiera-S | âœ… | **Best** | [ğŸ“¥ Download](https://drive.google.com/file/d/1JVmZnpWip7AIi8o9J1heog_Kl5uHGHcP/view?usp=sharing) |
| MedSAM2 | SAM2-Hiera-T | âœ… | Good | [ğŸ“¥ Download](https://drive.google.com/file/d/1XQmJ13-SahH-57eH1-UabU1OpGpoTZWT/view?usp=sharing) |
| MedSAM | SAM-ViT-B | âœ… | Good | [ğŸ“¥ Download](https://drive.google.com/file/d/1V81_3KuJ-7q1gzLYcQFPCTAAymfkxh6Y/view?usp=sharing) |
| SAM2 | SAM2-Hiera-S | âŒ | Baseline | [ğŸ“¥ Download](https://drive.google.com/file/d/1bNtsqOCRnzDOb_10EN9bAACLPew32yus/view?usp=sharing) |
| SAM | SAM-ViT-B | âŒ | Baseline | [ğŸ“¥ Download](https://drive.google.com/file/d/1LgRKsBkCYOeQQRWyF1RnXZgwe-_xfR0_/view?usp=sharing) |

</div>

> ğŸ“ Place downloaded weights in `./checkpoints/`

#### ğŸ’¾ Memory Bank (Required for SAMed-2)
Download the pre-trained memory bank: [**memory_bank_list_640.pkl**](https://drive.google.com/file/d/1nrq9GRhlCUG7ha-RuuQktODyfK1UKbwL/view?usp=sharing)  
Place it in the root directory of this repository.

### 4. Quick Start

<table>
<tr>
<td>

**ğŸš€ Simple Python API**

```python
from predict import MedicalSegmenter

# Initialize
segmenter = MedicalSegmenter(
    model_type='samed2',
    checkpoint_path='checkpoints/latest_epoch_0217.pth'
)

# Segment
result = segmenter.predict(
    'medical_image.png', 
    box=[100, 100, 900, 900]
)

# Visualize
segmenter.visualize(
    'medical_image.png', 
    result['mask'], 
    'result.jpg'
)
```

</td>
<td>

**ğŸ“Š Benchmark All Models**

```bash
# Test on OpticCup dataset
python main.py \
    -net samed2 \
    -exp_name OpticCup \
    -sam_ckpt checkpoints/latest_epoch_0217.pth \
    -sam_config sam2_hiera_s \
    -image_size 1024 \
    -b 4 \
    -data_path "/path/to/Data" \
    -val_file_dir "/path/to/OpticCup/test.txt"

# Or run complete benchmark
bash run.sh
```

</td>
</tr>
</table>

### 5. Evaluation

#### ğŸ“¦ Download Test Datasets

<div align="center">

| Dataset | Modality | Size | Download |
|:-------:|:--------:|:----:|:--------:|
| **Optic Cup** | Fundus | ~100MB | [ğŸ“¥ Download](https://drive.google.com/file/d/1jayJ9q627t6kNXNsacfW3b8i-oVPJ0wz/view?usp=sharing) |
| **Brain Tumor** | MRI | ~200MB | [ğŸ“¥ Download](https://drive.google.com/file/d/1WuJ8fD2stAqUKxYzsws2mMgS3M6JtFXK/view?usp=sharing) |

</div>

**Prepare Your Own Dataset**

For custom datasets, organize your data as follows:
```
your_dataset/
â”œâ”€â”€ image/
â”‚   â”œâ”€â”€ case_idx_slice_001.png
â”‚   â”œâ”€â”€ case_idx_slice_002.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ mask/
â”‚   â”œâ”€â”€ case_idx_slice_001.png
â”‚   â”œâ”€â”€ case_idx_slice_002.png
â”‚   â””â”€â”€ ...
â””â”€â”€ test.txt  # List of test image names
```

**Run Evaluation**

```bash
# Full evaluation on OpticCup dataset
bash run.sh
```

**Parameters Explanation:**
- `-net`: Model type (`samed2`, `medsam2`, `sam2`, `medsam`, `sam`)
- `-exp_name`: Dataset name for logging
- `-sam_ckpt`: Path to model checkpoint
- `-sam_config`: Configuration file
- `-image_size`: Input image size (default: 1024)
- `-out_size`: Output size (default: 1024)
- `-b`: Batch size
- `-data_path`: Root path to datasets
- `-train_file_dir`: Path to training file list
- `-val_file_dir`: Path to validation file list
- `-memory_bank_size`: Memory bank size for SAMed2 (default: 640)
- `-lr`: Learning rate (default: 1e-4)
- `-epoch`: Number of epochs (default: 100)

**ğŸ“ˆ Performance Comparison**

<div align="center">

| Dataset | SAM | MedSAM | SAM2 | MedSAM2 | **SAMed-2** |
|:-------:|:---:|:------:|:----:|:-------:|:-----------:|
| **OpticCup** | 0.61 | 0.86 | 0.62 | 0.40 | **0.90** ğŸ† |
| **BrainTumor** | 0.56 | 0.60 | 0.44 | 0.58 | **0.67** ğŸ† |

<sub>*Dice scores on test sets. Higher is better.*</sub>

</div>

## ğŸ® Demo

Try our interactive demo powered by SliceUI! [Demo Guide](docs/DEMO.md)

[![Demo Video](https://img.youtube.com/vi/DEMO_VIDEO_ID/0.jpg)](https://www.youtube.com/watch?v=DEMO_VIDEO_ID)

## ğŸ“š Citation

If you find SAMed2 useful in your research, please consider citing:

```bibtex
TODO
```

## ğŸ¤ Contributors

<div align="center">

[Zhiling YanÂ¹](https://scholar.google.com/citations?user=xxx) Â· 
[Sifan SongÂ²](https://scholar.google.com/citations?user=xxx) Â· 
[Dingjie SongÂ¹](https://scholar.google.com/citations?user=xxx) Â· 
[Yiwei LiÂ³](https://scholar.google.com/citations?user=xxx) Â· 
[Rong ZhouÂ¹](https://scholar.google.com/citations?user=xxx) Â· 
[Weixiang Sunâ´](https://scholar.google.com/citations?user=xxx)  
[Zhennong ChenÂ²](https://scholar.google.com/citations?user=xxx) Â· 
[Sekeun KimÂ²](https://scholar.google.com/citations?user=xxx) Â· 
[Hui RenÂ²](https://scholar.google.com/citations?user=xxx) Â· 
[Tianming LiuÂ³](https://scholar.google.com/citations?user=xxx) Â· 
[Quanzheng LiÂ²](https://scholar.google.com/citations?user=xxx)  
[Xiang LiÂ²](https://scholar.google.com/citations?user=xxx) Â· 
[Lifang HeÂ¹](https://scholar.google.com/citations?user=xxx) Â· 
[Lichao SunÂ¹](https://scholar.google.com/citations?user=xxx)

<sub>
Â¹Lehigh University &nbsp;&nbsp;|&nbsp;&nbsp; Â²Massachusetts General Hospital and Harvard Medical School  
Â³University of Georgia, Athens &nbsp;&nbsp;|&nbsp;&nbsp; â´University of Notre Dame
</sub>

</div>

## ğŸ™ Acknowledgements

We gratefully acknowledge:
- **[SAM2](https://github.com/facebookresearch/segment-anything-2)** - The foundation model we build upon
- **[MedSAM](https://github.com/bowang-lab/MedSAM)** - Inspiration for medical adaptation
- **[SliceUI](https://github.com/yourusername/sliceUI)** - Interactive demo interface 

